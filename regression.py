# -*- coding: utf-8 -*-
"""Regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QIUW4B0JakJxt3Wmy4EjpyTaNpU_YK2v
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

import math
import time
import numpy as np
import pandas as pd
import seaborn as sns
import tensorflow as tf
from keras.layers import LSTM
import matplotlib.pyplot as plt
from keras.models import Sequential
from keras.layers import Dense, Activation
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import MinMaxScaler
from datetime import date, datetime, time, timedelta

print(tf.__version__)

# Colab에서 kaggle 다운로드, kaggle API 업로드
!pip install kaggle
from google.colab import files
files.upload()

# ls -1ha kaggle.json
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/

# Persmisson Warning 방지
!chmod 600 ~/.kaggle/kaggle.json

# kaggle에서 다운로드
!kaggle datasets download -d artemburenok/sp-500-stock-prices

# zip으로 압축되어 있는지 확인하고, 압축 해제
!ls
!unzip sp-500-stock-prices.zip

def new_dataset(dataset, step_size):
	data_X, data_Y = [], []
	for i in range(len(dataset)-step_size-1):
		a = dataset[i:(i+step_size), 0]
		data_X.append(a)
		data_Y.append(dataset[i + step_size, 0])
	return np.array(data_X), np.array(data_Y)

column_names = ['Date', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']
raw_dataset = pd.read_csv('/content/AAPL.csv', names=column_names,
                      na_values = "?", comment='\t',
                      sep=",", skipinitialspace=True)

dataset = raw_dataset.copy()
# dataset, 위에서 부터 5개의 행 출력
dataset.head()

# index 0이 사실상 쓸모없는 값이라 제거
dataset = dataset.drop(0)
dataset.head()

# Object 타입을 실수형으로 변경
dataset['Date'] = pd.to_datetime(dataset['Date'])
dataset['Open'] = np.array(dataset['Open'], dtype='float64')
dataset['High'] = np.array(dataset['High'], dtype='float64')
dataset['Low'] = np.array(dataset['Low'], dtype='float64')
dataset['Close'] = np.array(dataset['Close'], dtype='float64')
dataset['Adj Close'] = np.array(dataset['Adj Close'], dtype='float64')
# dataset['Volume'] = np.array(dataset['Volume'], dtype='int64')

dataset.sort_values(by='Date', inplace=True, ascending=True)

# NULL 값 제거 
dataset.isna().sum()

# 신규 인덱스 생성
obs = np.arange(1, len(dataset)+1, 1)

# 예측에 사용될 지표 생성
OHLC_avg = dataset.mean(axis=1)
HLC_avg = dataset[['High', 'Low', 'Close']].mean(axis=1)
close_val = dataset[['Close']]

# 그래프로 각 지표들을 표시
plt.plot(obs, OHLC_avg, 'r', label = 'OHLC avg')
plt.plot(obs, HLC_avg, 'b', label = 'HLC avg')
plt.plot(obs, close_val, 'g', label = 'Closing price')
plt.legend(loc = 'upper right')
plt.show()

# 시계열 데이터로 준비
OHLC_avg = np.reshape(OHLC_avg.values, (len(OHLC_avg), 1))
scaler = MinMaxScaler(feature_range=(0, 1))
OHLC_avg = scaler.fit_transform(OHLC_avg)

# 데이터 셋을 훈련, 검증 셋으로 나눔.
train_OHLC = int(len(OHLC_avg) * 0.75)
test_OHLC = len(OHLC_avg) - train_OHLC
train_OHLC, test_OHLC = OHLC_avg[:train_OHLC,:], OHLC_avg[train_OHLC:len(OHLC_avg),:]

# 시계열 데이터로 변경
train_x, train_y = new_dataset(train_OHLC, 1)
test_x, test_y = new_dataset(test_OHLC, 1)

# Reshaping
train_x = np.reshape(train_x, (train_x.shape[0], 1, train_x.shape[1]))
test_x = np.reshape(test_x, (test_x.shape[0], 1, test_x.shape[1]))
step_size = 1

# 모델 생성, 객체로 생성하지 않음.
model = Sequential()
model.add(LSTM(32, input_shape=(1, step_size), return_sequences=True))
model.add(LSTM(16))
model.add(Dense(1))
model.add(Activation('linear'))

# 모델 컴파일, 모델 훈련
model.compile(loss='mean_squared_error', optimizer='adagrad')
model.fit(train_x, train_y, epochs=100, batch_size=1, verbose=2)

train_p = model.predict(train_x)
test_p = model.predict(test_x)

train_p = scaler.inverse_transform(train_p)
train_y = scaler.inverse_transform([train_y])

test_p = scaler.inverse_transform(test_p)
test_y = scaler.inverse_transform([test_y])

train_score = math.sqrt(mean_squared_error(train_y[0], train_p[:,0]))
print('Train RMSE: %.2f' % (train_score))

test_score = math.sqrt(mean_squared_error(test_y[0], test_p[:,0]))
print('Test RMSE: %.2f' % (test_score))

train_p_plot = np.empty_like(OHLC_avg)
train_p_plot[:, :] = np.nan
train_p_plot[step_size : len(train_p)+step_size, :] = train_p

test_p_plot = np.empty_like(OHLC_avg)
test_p_plot[:, :] = np.nan
test_p_plot[len(train_p)+(step_size * 2)+1 : len(OHLC_avg)-1, :] = test_p

OHLC_avg = scaler.inverse_transform(OHLC_avg)

plt.plot(OHLC_avg, 'g', label = 'original dataset')
plt.plot(train_p_plot, 'r', label = 'training set')
plt.plot(test_p_plot, 'b', label = 'predicted stock price/test set')
plt.legend(loc = 'upper right')
plt.xlabel('Time in Days')
plt.ylabel('OHLC Value of Apple Stocks')
plt.show()

last_val = test_p[-1]
last_val_scaled = last_val/last_val
next_val = model.predict(np.reshape(last_val_scaled, (1,1,1)))
print("Last Day Value:", np.asscalar(last_val))
print("Next Day Value:", np.asscalar(last_val*next_val))